{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "from tensorflow.python.ops import array_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "\n",
    "    ic = tf.matmul(i, array_ops.concat(concat_dim=1, values=[ix,fx,cx,ox]))\n",
    "    oc = tf.matmul(o, array_ops.concat(concat_dim=1, values=[im,fm,cm,om]))\n",
    "    \n",
    "    iix, ifx, icx, iox = array_ops.split(1, 4, ic)\n",
    "    oim, ofm, ocm, oom = array_ops.split(1, 4, oc)\n",
    "    \n",
    "    input_gate = tf.sigmoid(iix + oim + ib)\n",
    "    forget_gate = tf.sigmoid(ifx + ofm + fb)\n",
    "    output_gate = tf.sigmoid(iox + oom + ob)\n",
    "    \n",
    "    update = icx + ocm + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294579 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.97\n",
      "================================================================================\n",
      "u lrpfmemgc c xr mnn  ihioo tetrioteveifn  hip ijd fn qmegfxiub rzsidbvrjpzcnnre\n",
      "ljh  jpla  tvhcwxa  uuz   a ecmm skztiusngvbsocrbscf grnucgjo giuotmnf mldre on \n",
      "ravf  eraens  mjdalss pabfdzaq  hcu rflkrn chvuc deayhtyehfcboo fcxzucqabea ek f\n",
      "sova e t  vdendiowsnczts fgeojhlid we  tregpih pl rrakfa pvkug f cqccia  jtuhbgw\n",
      "vt e xapog hlehgxqwalajiw vcfcfaareohovdtf itexyibxa nxdxtjfpizraqmcyttnntu ekia\n",
      "================================================================================\n",
      "Validation set perplexity: 20.31\n",
      "Average loss at step 100: 2.584798 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.93\n",
      "Validation set perplexity: 10.27\n",
      "Average loss at step 200: 2.242994 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.49\n",
      "Validation set perplexity: 8.42\n",
      "Average loss at step 300: 2.100955 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.38\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 400: 1.998445 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 500: 1.938014 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 600: 1.910472 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 700: 1.865379 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 800: 1.824399 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 900: 1.835527 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 1000: 1.828792 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "================================================================================\n",
      "mey meecanic before for a or aphel becello feodrc sits in bepolitly of from to k\n",
      "cone fre frog mabors brermect of itmen setwored that that undiol of uvead joq it\n",
      "ch presits of the on of the urecicke anvamber afian dione new come leder ene lev\n",
      "quatay farsm aberowhe b hedations appoprict in ode becolic of moatil dishing tha\n",
      "que his dester two equemp is curres new of of the fiendy mecore nive new secuer \n",
      "================================================================================\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 1100: 1.780430 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 1200: 1.758443 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1300: 1.739767 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1400: 1.752538 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1500: 1.743103 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1600: 1.753648 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1700: 1.718720 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1800: 1.678421 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1900: 1.656448 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2000: 1.704217 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "================================================================================\n",
      "y the konjo from lassensergic and who notts nembonining the used have cityt the \n",
      "led inthrodord delit s in sblike dogramative ghought son smald the sets constity\n",
      " prpsidek the onchilm to the idersed some instird ls uslins shown hogetacted ser\n",
      "y s yill aid three with dby name selact in concoditions whise is secusutuives gr\n",
      "new to a rengerue experss sy one nine five sfelition to three one six six four y\n",
      "================================================================================\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2100: 1.692803 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2200: 1.689410 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2300: 1.646308 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2400: 1.666048 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2500: 1.682819 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2600: 1.663862 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2700: 1.665520 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2800: 1.656259 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2900: 1.654972 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3000: 1.658952 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "urly in the side carles arictanes of the lacking uside government of apaper s ge\n",
      "def alsusssong consponkna mater traditional later p based and his the ord also e\n",
      "untine a whot elom the thurgn wor inclust inscrate and i and is marain ecorated \n",
      "vide a with isfover us this clayer bason of thipe augal rus the sactecture pabli\n",
      "land betone calles dectiful whereth called linales culls of that sovertions nust\n",
      "================================================================================\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3100: 1.632629 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3200: 1.650931 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3300: 1.639637 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3400: 1.669499 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3500: 1.658865 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3600: 1.672614 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3700: 1.648750 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3800: 1.648436 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3900: 1.640225 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4000: 1.655770 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "que of the use of the bladrow and colmertest real peagateres and most indover ha\n",
      "his econdections hens shidew mmperal prince saters edvet an off one three one ni\n",
      " beer innuered hesck the termusian an farres ferems sometime was defleman i one \n",
      " french desequide stant later the or solmators primes is the was page of the ess\n",
      "que somevart and natherius poadition or also cities of grannes be iperomber note\n",
      "================================================================================\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4100: 1.637673 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4200: 1.638634 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4300: 1.616129 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4400: 1.613397 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4500: 1.615239 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4600: 1.614591 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4700: 1.627012 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4800: 1.634551 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4900: 1.633918 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5000: 1.608815 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "================================================================================\n",
      "ge for denaty dane wakes acholopts every eight who three zero zero s revel of br\n",
      "ock so humge becauled the classited go doem a sellation the pagom initated he st\n",
      "vinger reast and the ip the wnot condide from from the note the two celve arthir\n",
      "as or upinenop ancivilion shiang uson and coinving of the penalty the literation\n",
      "utions one a vart banksting concereic of secrelance i br necestmination one six \n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5100: 1.607750 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5200: 1.592925 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300: 1.580728 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5400: 1.581096 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5500: 1.567171 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5600: 1.582216 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5700: 1.573179 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5800: 1.581573 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5900: 1.577043 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6000: 1.549028 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "plaw the molectic of has shortly delawer stace with the store a link one two pob\n",
      "var for the rune one nine five elinglation eishderist vars of the statier strict\n",
      "hoking however plays and player played there councilly drampelise in histo playe\n",
      "wockar by truding with aucotology s the oppopeinies two two they with theoring e\n",
      "cings there their tistomern after hombin dice has propose but in the held othan \n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6100: 1.566502 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6200: 1.536914 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6300: 1.548166 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6400: 1.543274 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6500: 1.558861 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6600: 1.602677 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6700: 1.582453 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6800: 1.607294 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6900: 1.582999 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 7000: 1.581209 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "d boll and theoregic one nine one three seven likely day and which junwix quits \n",
      "jelizander hus seven userg and coblicsed hossiold than befic of one nine eight s\n",
      "jo new this ham and counte a few resige have he freacanda two zero zero a intron\n",
      "uthly historian ambihomination shows schoi results on their off cupnishle fifble\n",
      "x new leanon the word the gorol and williblanged proples the fead anto capboriat\n",
      "================================================================================\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 7100: 1.575119 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 7200: 1.574629 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 7300: 1.572738 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.98\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 7400: 1.587207 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 7500: 1.593643 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 7600: 1.555801 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 7700: 1.552209 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 7800: 1.579123 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 7900: 1.583494 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.89\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 8000: 1.619544 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "================================================================================\n",
      "red geneace typilation hodatite sest of spieb hary p n s parenet a comparing gec\n",
      "histe been the zero five eight nine seven four seven four one maleflay dependire\n",
      "use arexwing the grand is a karcurine a fenitul one nine seven four seven one se\n",
      "houjopmely of second bs the final networt book ushop a send at x the four histor\n",
      "x sofie late carmation with were flazeld prepersions the used rochoole in jadit \n",
      "================================================================================\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 8100: 1.600675 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 8200: 1.572415 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 8300: 1.570128 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 8400: 1.584404 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 8500: 1.586602 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 8600: 1.577257 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 8700: 1.573149 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 8800: 1.544504 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 8900: 1.559717 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 9000: 1.556131 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      " six xing monen is publige eantical followrehd ilish is all beery interly othis \n",
      "stronaud four united of characted even hat nations of the cennions the and excal\n",
      "jets increney one nine four one organts southson skiles defenber for a to her du\n",
      "an petabi can athabitly sigise over african perto appeared moneved over pact his\n",
      "dish trad siffented frenchushne the batter megray fiction in respan sime in may \n",
      "================================================================================\n",
      "Validation set perplexity: 4.26\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "    \n",
    "      #print (predictions.shape, predictions)\n",
    "      #print (labels.shape, labels)\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_lstm_cell(i, o, state):\n",
    "\n",
    "    ic = tf.matmul(i, array_ops.concat(concat_dim=1, values=[ix,fx,cx,ox]))\n",
    "    oc = tf.matmul(o, array_ops.concat(concat_dim=1, values=[im,fm,cm,om]))\n",
    "    \n",
    "    iix, ifx, icx, iox = array_ops.split(1, 4, ic)\n",
    "    oim, ofm, ocm, oom = array_ops.split(1, 4, oc)\n",
    "    \n",
    "    input_gate = tf.sigmoid(iix + oim + ib)\n",
    "    forget_gate = tf.sigmoid(ifx + ofm + fb)\n",
    "    output_gate = tf.sigmoid(iox + oom + ob)\n",
    "    \n",
    "    update = icx + ocm + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import compress\n",
    "from IPython import display\n",
    "from nltk import bigrams\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import math\n",
    "\n",
    "def bigram(text, overlap=False):\n",
    "    \"\"\"Convert a string of text to a list of bigrams\"\"\"\n",
    "    bg = bigrams(text)\n",
    "    if overlap: return [''.join(t) for t in bg]\n",
    "    else: return [''.join(t) for i, t in enumerate(bg) if i%2==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000000\n"
     ]
    }
   ],
   "source": [
    "bg = bigram(text)\n",
    "print(len(bg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' a', 'na', 'rc', 'hi', 'sm', ' o', 'ri', 'gi', 'na', 'te', 'd ', 'as', ' a']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg[:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 729\n",
      "Most common words (+UNK) [['UNK', 0], ('e ', 1843425), (' t', 1224131), ('s ', 1111188), ('th', 990343)]\n",
      "Sample data [5, 96, 220, 75, 267, 10, 56, 196, 96, 29]\n"
     ]
    }
   ],
   "source": [
    "voc_size = 75000\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(voc_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "datum, count, dictionary, reverse_dictionary = build_dataset(bg)\n",
    "voc_size = len(dictionary)\n",
    "\n",
    "print('Dictionary size:', voc_size)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', datum[:10])\n",
    "\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_batch_skipgram(dat, batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span) # used for collecting data[data_index] in the sliding window\n",
    "    for _ in range(span):\n",
    "            buffer.append(dat[data_index])\n",
    "            data_index = (data_index + 1) % len(dat)\n",
    "    for i in range(batch_size // num_skips):\n",
    "            target = skip_window  # target label at the center of the buffer\n",
    "            targets_to_avoid = [ skip_window ]\n",
    "            for j in range(num_skips):\n",
    "                while target in targets_to_avoid:\n",
    "                    target = random.randint(0, span - 1)\n",
    "                targets_to_avoid.append(target)\n",
    "                batch[i * num_skips + j] = buffer[skip_window]\n",
    "                labels[i * num_skips + j, 0] = buffer[target]\n",
    "            buffer.append(dat[data_index])\n",
    "            data_index = (data_index + 1) % len(dat)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "architecture = 'SKIP'\n",
    "\n",
    "batch_size = 512\n",
    "embedding_size = 64 # Dimension of the embedding vector.\n",
    "skip_window = 4 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "if architecture == 'CBOW':\n",
    "    input_shape = [batch_size, num_skips]\n",
    "    generate_batch = generate_batch_cbow\n",
    "elif architecture == 'SKIP':\n",
    "    input_shape = [batch_size]\n",
    "    generate_batch = generate_batch_skipgram\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "  # Input data.\n",
    "  train_dataset = tf.placeholder(tf.int32, shape=input_shape)\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "  softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([voc_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([voc_size]))\n",
    "  \n",
    "  if architecture == 'SKIP':\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    \n",
    "  elif architecture == 'CBOW':\n",
    "    emb = tf.zeros([batch_size, embedding_size])\n",
    "    for j in range(num_skips):\n",
    "        emb += tf.nn.embedding_lookup(embeddings, train_dataset[:, j])\n",
    "    embed = emb\n",
    "\n",
    "  loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,\n",
    "                                                   train_labels, num_sampled, voc_size))  \n",
    "\n",
    "  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "  \n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "    normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 50000: 3.821967\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0QAAAJoCAYAAABGACHXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3c23ab24d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to es: ns, bv, gq, ep, oj, ed, ms, zb,\n",
      "Nearest to si: qf, xz, kj, sn, xr, kx, mc, mo,\n",
      "Nearest to  h: yj, sz, nw,  w, zy, gp,  d, eh,\n",
      "Nearest to l : c , kp, fn, mk, a , wk, n , d ,\n",
      "Nearest to om: am, jn, ol, ym, ko, im, sv, ap,\n",
      "Nearest to el: zf, eg, ua, tv, il,  d, ec, wj,\n",
      "Nearest to  r:  d, tk, vf, pb, xd, cc, zq, pr,\n",
      "Nearest to fi: ze, pe, tk, ni, us, tl, fe, no,\n",
      "Nearest to  i: qs, jy, fx, yu, xf, za,  c, ha,\n",
      "Nearest to  b: nl, pb, tl, vy, kx,  m, ll, dc,\n",
      "Nearest to ca: ga, vo, gi, lx, co, na, vv, ve,\n",
      "Nearest to en: ih, nm, mz, pk, jz, qr, dj, tq,\n",
      "Nearest to  a: sn, pz, xm, xp, oy, lz, oe, ef,\n",
      "Nearest to UNK: go, su, cu, wr, fs, jl, qp, lv,\n",
      "Nearest to  p:  v,  c,  g, wm,  m, kc, ep, yi,\n",
      "Nearest to ar: os, rf, uv, ag, rl, la, cg, oj,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 50001\n",
    "lh = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "        \n",
    "    batch_data, batch_labels = generate_batch(datum, batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    \n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    \n",
    "    average_loss += l\n",
    "    \n",
    "    if step % 2000 == 0:\n",
    "      display.clear_output(wait=True)\n",
    "      if step > 0:\n",
    "        average_loss = average_loss / 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step %d: %f' % (step, average_loss))\n",
    "      lh.append(average_loss)\n",
    "      average_loss = 0\n",
    "            \n",
    "      plt.figure(figsize=(14, 10))\n",
    "\n",
    "      plt.title(\"Training loss, step {}/{}\".format(step, num_steps))\n",
    "      plt.xlabel(\"#step\")\n",
    "      plt.ylabel(\"loss\")\n",
    "      plt.plot(lh, 'b')\n",
    "      plt.show()\n",
    "      \n",
    "\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log = '%s %s,' % (log, close_word)\n",
    "        print(log)\n",
    "        \n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BI_EMB = final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('BIEMB.pkl','wb') as f:\n",
    "    pkl.dump(BI_EMB, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_unrollings=10\n",
    "\n",
    "class MyBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, embedding_size), dtype=np.float)\n",
    "    n = int(self._batch_size)\n",
    "\n",
    "    for b in range(n):\n",
    "      bigram =  self._text[self._cursor[b]]\n",
    "      batch[b, :] = BI_EMB[dictionary[bigram]]\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def toCharacters(probabilities):\n",
    "    \"\"\"Turn an embedding into its (most likely) character representation.\"\"\"\n",
    "    return [reverse_dictionary[t] for t in np.argmax(cosine_similarity(BI_EMB, probabilities), 0)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, toCharacters(b))]\n",
    "  return s\n",
    "\n",
    "def my_sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, embedding_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def my_random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, embedding_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "btt = bigram(train_text)\n",
    "bvt = bigram(valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Lstm(object):\n",
    "\n",
    "    def __init__(self, batch_size, embeddings, num_nodes, num_unrollings,\n",
    "                 temperature=0.2, graph=None):\n",
    "        self._graph = graph or tf.get_default_graph()\n",
    "        self._batch_size = batch_size\n",
    "        self._num_nodes = num_nodes\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._embeddings = embeddings.embeddings_tensor()\n",
    "        self._embedding_size = embeddings.embedding_size\n",
    "        self._temperature = temperature\n",
    "        self._scope_name = \"lstm_\" + str(id(self))\n",
    "        with tf.variable_scope(self._scope_name):\n",
    "            self._define_lstm()\n",
    "\n",
    "    def _define_parameters(self):\n",
    "        self._embeddings = tf.Variable(self._embeddings, trainable=False)\n",
    "\n",
    "        self._saved_output = tf.Variable(\n",
    "            tf.zeros([self._batch_size, self._num_nodes]), trainable=False)\n",
    "        self._saved_state = tf.Variable(\n",
    "            tf.zeros([self._batch_size, self._num_nodes]), trainable=False)\n",
    "\n",
    "        self._w = tf.Variable(tf.truncated_normal([self._num_nodes,\n",
    "                                                   self._embedding_size], -0.1, 0.1))\n",
    "        self._b = tf.Variable(tf.zeros([self._embedding_size]))\n",
    "\n",
    "        self._iW = tf.Variable(tf.truncated_normal(\n",
    "            [self._embedding_size, self._num_nodes * 4], -0.1, 0.1))\n",
    "        self._oW = tf.Variable(tf.truncated_normal(\n",
    "            [self._num_nodes, self._num_nodes * 4], -0.1, 0.1))\n",
    "        self._B = tf.Variable(tf.zeros([1, self._num_nodes * 4]))\n",
    "\n",
    "    def _define_inputs(self):\n",
    "        self._train_data = []\n",
    "        self._embedded_train_data = []\n",
    "        for _ in range(self._num_unrollings + 1):\n",
    "            batch = tf.placeholder(tf.int32, shape=[self._batch_size])\n",
    "            self._train_data.append(batch)\n",
    "            embedded_batch = tf.nn.embedding_lookup(self._embeddings, batch)\n",
    "            self._embedded_train_data.append(embedded_batch)\n",
    "\n",
    "        self._train_inputs = self._embedded_train_data[:self._num_unrollings]\n",
    "        self._train_labels = self._embedded_train_data[1:]\n",
    "\n",
    "    def _define_lstm_chain(self):\n",
    "        self._outputs = []\n",
    "        output = self._saved_output\n",
    "        state = self._saved_state\n",
    "        for i in self._train_inputs:\n",
    "            output, state = self._lstm_cell(i, output, state)\n",
    "            self._outputs.append(output)\n",
    "\n",
    "        with tf.control_dependencies([self._saved_output.assign(output),\n",
    "                                      self._saved_state.assign(state)]):\n",
    "            self._logits = tf.nn.xw_plus_b(\n",
    "                tf.concat(0, self._outputs), self._w, self._b)\n",
    "            self._loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                self._logits, tf.concat(0, self._train_labels)))\n",
    "\n",
    "    def _define_sample_output(self):\n",
    "        self._sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "        self._embedded_sample_input = tf.nn.embedding_lookup(\n",
    "            self._embeddings, self._sample_input)\n",
    "        saved_sample_output = tf.Variable(tf.zeros([1, self._num_nodes]))\n",
    "        saved_sample_state = tf.Variable(tf.zeros([1, self._num_nodes]))\n",
    "        reset_sample_state = tf.group(\n",
    "            saved_sample_output.assign(tf.zeros([1, self._num_nodes])),\n",
    "            saved_sample_state.assign(tf.zeros([1, self._num_nodes])))\n",
    "        sample_output, sample_state = self._lstm_cell(\n",
    "            self._embedded_sample_input, saved_sample_output, saved_sample_state)\n",
    "        with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                      saved_sample_state.assign(sample_state)]):\n",
    "            self._sample_embedded_prediction = tf.nn.softmax(\n",
    "                tf.nn.xw_plus_b(sample_output, self._w, self._b))\n",
    "            diff = self._embeddings - self._sample_embedded_prediction\n",
    "            distance = tf.sqrt(tf.reduce_sum(diff ** 2, 1))\n",
    "            inverse = (tf.reduce_max(distance) - distance) / self._temperature\n",
    "            prediction = tf.nn.softmax(tf.expand_dims(inverse, 0))\n",
    "            self._sample_prediction = tf.squeeze(prediction)\n",
    "\n",
    "    def _define_lstm(self):\n",
    "        with self._graph.as_default():\n",
    "            self._define_parameters()\n",
    "\n",
    "            self._define_inputs()\n",
    "\n",
    "            self._define_lstm_chain()\n",
    "\n",
    "            global_step = tf.Variable(0)\n",
    "            self._learning_rate = tf.train.exponential_decay(\n",
    "                10.0, global_step, 5000, 0.1, staircase=True)\n",
    "            self._optimizer = tf.train.GradientDescentOptimizer(\n",
    "                self._learning_rate)\n",
    "            gradients, v = zip(*self._optimizer.compute_gradients(self._loss))\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "            self._optimizer = self._optimizer.apply_gradients(\n",
    "                zip(gradients, v), global_step=global_step)\n",
    "\n",
    "            self._train_prediction = tf.nn.softmax(self._logits)\n",
    "\n",
    "            self._define_sample_output()\n",
    "\n",
    "    def _lstm_cell(self, i, o, state):\n",
    "        gate = tf.matmul(i, self._iW) + tf.matmul(o, self._oW) + self._B\n",
    "        input_gate = tf.sigmoid(gate[:, 0:self._num_nodes])\n",
    "        forget_gate = tf.sigmoid(gate[:, self._num_nodes:2 * self._num_nodes])\n",
    "        update = tf.tanh(gate[:, 2 * self._num_nodes:3 * self._num_nodes])\n",
    "        output_gate = tf.sigmoid(gate[:, 3 * self._num_nodes:])\n",
    "        state = forget_gate * state + input_gate * update\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    def train(self, session, text, num_steps):\n",
    "        summary_frequency = 100\n",
    "        is_own = lambda x: x.name.startswith(self._scope_name)\n",
    "        tf.initialize_variables(filter(is_own, tf.all_variables())).run()\n",
    "        print('Initialized')\n",
    "        mean_loss = 0\n",
    "        generator = bigram_batch.BigramGenerator(text, self._batch_size,\n",
    "                                                 num_unrollings=self._num_unrollings)\n",
    "        for step in range(num_steps):\n",
    "            batches = generator.next()\n",
    "            feed_dict = dict()\n",
    "            for i in range(self._num_unrollings + 1):\n",
    "                feed_dict[self._train_data[i]] = batches[i]\n",
    "            _, l, predictions, lr = session.run(\n",
    "                [self._optimizer, self._loss,\n",
    "                 self._train_prediction, self._learning_rate], feed_dict=feed_dict)\n",
    "            mean_loss += l\n",
    "            if step % summary_frequency == 0:\n",
    "                if step > 0:\n",
    "                    mean_loss = mean_loss / summary_frequency\n",
    "                # The mean loss is an estimate of the loss over the last few\n",
    "                # batches.\n",
    "                print(\n",
    "                    'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "\n",
    "    def say(self, length, start_from=None):\n",
    "        bigram_id = start_from or random.randint(\n",
    "            0, bigram_batch.vocabulary_size - 1)\n",
    "        text = bigram_batch.id2bigram(bigram_id)\n",
    "\n",
    "        def sample(distribution):\n",
    "            r = random.uniform(0, 1)\n",
    "            s = 0\n",
    "            for i in range(len(distribution)):\n",
    "                s += distribution[i]\n",
    "                if s >= r:\n",
    "                    return i\n",
    "            return len(distribution) - 1\n",
    "\n",
    "        for _ in range(length):\n",
    "            prediction = self._sample_prediction.eval(\n",
    "                {self._sample_input: [bigram_id]})\n",
    "            bigram_id = sample(prediction)\n",
    "            text += bigram_batch.id2bigram(bigram_id)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
