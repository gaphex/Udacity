{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "8tQJd2YSCfWR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "from tensorflow.python.ops import array_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [],
   "source": [
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "num_unrollings=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'bout it in ', 'nomination ', 'lower here ', 'when milita', 'ogenetic ap', ' three nine', 'unced that ', 'lleria arch', 'any he did ', 'reviated as', 'unce the wo', ' abbeys and', 'one a a gen', 'shing the r', 's of it to ', 'married urr', 'ouncil twic', 'sity upset ', 'toon advent', 'hel and ric', 't used the ', 'ased in the', 'settled the', 'y and litur', 'nes flight ', ' disgust be', 'tury the wa', 'ay opened f', ' were a tri', 'society and', 'hree mm zer', 'tion from t', 'y the human', 'ago based c', 'each the dh', 'migration t', 's one zero ', ' zero zero ', 'rick and al', 'new york ot', 'ycles have ', 'short subje', 'sample code', 'he boeing s', 'em a year l', 'sgow two yo', 'ne nine eig', 'e listed wi', 'ill even mo', 'lt during t', 'ine one it ', 'eber has pr', ' x three an', ' not dead n', 'f the team ', 'o be made t', 'hy essentia', 'll s enthus', 'unds the mo', 'yer who rec', ' a step or ', 'operates th', 's has been ', 'ore signifi', ' community ', 'rmines secu', 'se were pub', 'a fierce cr', 'often used ', ' fuel extra', 'h are used ', ' two six ei', 'tland which', 'ature that ', 'e three two', 'aristotle s', ' about one ', 'e dragas co', 'hree five f', 'ity can be ', ' popularity', 'ecombinant ', 're both wor', ' and intrac', 'believed to', 'tensive man', 'established', 'tion of the', 'sored by th', 'he attack f', 'l elements ', 'dy to pass ', 'the number ', 'ed to bring', 'king the en', 'f certain d', 'yed in ezek', 'french jans', 'vance gba d', 'at it will ', 'racter dies', 'tion from e', 'inating it ', 'e convince ', 'm the john ', 'ither spont', 'their praye', 'ent told hi', 'ee six whic', 'argest part', 'jebas balen', 'ampaign and', 'wo zero th ', 'ce in a spe', 'aryotic org', 'rver side s', ' ripening o', 'gain the am', 'he agency t', 'ious texts ', 'ands entire', ' assignment', 'hifley labo', 'o capitaliz', 'is pronounc', 'rettas fran', ' other exot', 'a duplicate', ' disks with', 'former is w', ' two six on', 'gh ann es d', 'in a song f', 'g the serie', 'such as rid', 'ine january', 'g should be', 'ar it is po', 'ections of ', 'ross zero t', ' for exampl', 's worldwide', 'y knowledge', 'cal theorie', 'ro ad by co', 'f an inch t', 'nds ed robe', 'ast instanc', 'ends to des', 'e phantom a', 'gives out t', ' dimensiona', 'ithdrew bra', 'as pi appro', ' was a tire', 'most holy m', 'uman story ', 'fice did no', ' telecommun', 't s support', 'iod of tran', 'olunteers o', 'melville s ', 'u is still ', 'tures in on', 'e end of wo', ' campaignin', 'e oscillati', ' zero zero ', 'tain its su', 'ed on colon', 'o eight sub', 'chusetts me', 'normal cour', 'od of war e', 'of italy la', 'ith which h', 'e of the le', ' software p', 's the tower', 'and written', 'e icj repor', 'respond exa', 'klahoma pre', 'ar kinds of', ' most of th', 'ed him to c', 'erprise lin', 'e six large', ' running of', 'ica parts o', 'ws becomes ', 'ine six nin', 'lization ab', 'in february', 'et in a naz', 'e seven the', 'ey accepted', 'ife of grod', 'the fabian ', 'an t be see', ' help when ', 'ons this pr', 'etchy to re', 'n systems a', 'ng bombs wi', 'press robin', ' sharman ne', ' robbe gril', 'laces for k', ' long barre', 'ised empero', 'liably be u', 'erican foot', 'factor but ', 'ting in pol', 'four zero f', 'nd has been', 'he theory o', 'd neo latin', 'erger one n', 'seco the a ', 'rong was a ', 'th risky ri', 'they hear g', 'er donna tr', 'm paramedic', 'encyclopedi', 'ndows also ', 'rall defini', 'city is one', 'fense the a', 'struction a', 'ecause of c', 'three one b', 'duating fro', ' grappling ', 'ifferent fo', ' doomed it ', 'treet grid ', 'r four zero', 'n militant ', 'or by food ', 'ations more', 'studies the', 'etal compou', ' in show bu', 'appeal of d', ' types of m', 'rs larry an', 'me three of', 'si have mad', ' called cla', 'ncluding em', 'ficult to c']\n",
      "['ists advoca', ' part two c', ' gore s end', ' the whole ', 'ary governm', 'pproach two', 'e one six z', ' atheism wo', 'hes nationa', ' not protes', 's dr mr and', 'ord america', 'd monasteri', 'neral inver', 'right of ap', ' make the d', 'raca prince', 'ce in their', ' the devils', 'tures had p', 'chard baer ', ' rom routin', 'e st family', 'e arctic fo', 'rgical lang', ' six two fi', 'ecause of t', 'aters were ', 'for passeng', 'ibe of sout', 'd that this', 'ro five in ', 'the nationa', 'ns ash init', 'chess recor', 'harma he wa', 'took place ', ' zero zero ', ' five yaniv', 'lfred rosen', 'ther well k', ' small whee', 'ect college', 'e for c and', 'seven six s', 'later moore', 'oung white ', 'ght zero s ', 'ith a gloss', 'ore content', 'this period', ' was known ', 'robably bee', 'nd x one or', 'naturally a', ' s success ', 'to recogniz', 'ally elemen', 'siastic bac', 'ost promine', 'ceived the ', ' two toward', 'hree submar', ' enjoyed an', 'icant than ', ' college ne', 'urity of th', 'blished as ', 'ritic of th', ' in a more ', 'acted from ', ' to this da', 'ight in sig', 'h then beca', ' was attack', 'o four ocke', 's uncaused ', ' zero zero ', 'onstantine ', 'four two vo', ' lost as in', 'y and moral', ' region and', 'rking on de', 'cellular ic', 'o be import', 'nufacturing', 'd lyell s c', 'e size of t', 'he russell ', 'from hyrsyl', ' or trachei', ' him a stic', ' q by one p', 'g good fort', 'nd of his p', 'drugs confu', 'kiel two ei', 'senist theo', 'dragon ball', ' take to co', 's all items', 'euclidean g', ' and the me', ' the priest', ' connally u', 'taneously o', 'ers a force', 'im to name ', 'ch became t', 'tner of the', 'ngues and b', 'd barred at', ' century ea', 'ecial cell ', 'ganelles ma', 'standard fo', 'of fruit th', 'mplified si', 'to change t', ' such as es', 'ely upon th', 't of number', 'or governme', 'ze on the g', 'ced and tha', 'ncis poulen', 'tic technol', 'e of the or', 'h questiona', 'widely used', 'ne nine nin', 'd hiver one', 'for sinatra', 'es to a sev', 'ddles child', 'y eight mar', 'e moved so ', 'ossible the', ' this artic', 'the lead ch', 'le a carrie', 'e after eng', 'e of the em', 'es classica', 'onstantine ', 'they were f', 'ert silverb', 'ce the non ', 'scribe deat', 'appear on s', 'the award i', 'al analysis', 'andenburg f', 'ox frac the', 'eless worke', 'mormons bel', ' from histo', 'ot become f', 'nications i', 't or at lea', 'nsformation', 'originally ', ' lifetime e', ' disagreed ', 'ne nine two', 'orld war ii', 'ng in recog', 'ing system ', ' five isbn ', 'urface area', 'nial troops', 'btypes base', 'erely looke', 'rse of stud', 'eventually ', 'anguages th', 'he or she a', 'eadership a', 'patent worl', 'r commissio', 'n after the', 'rts seven s', 'actly to th', 'ess one nin', 'f insurance', 'he indictee', 'carefully o', 'nux suse li', 'est cities ', 'f the house', 'of kingston', ' the first ', 'ne get back', 'bout the pe', 'y one six e', 'zi concentr', 'e country y', 'd a proposa', 'dno and lit', ' society ne', 'en the wond', ' the queen ', 'revented th', 'elatively s', 'according t', 'ith global ', 'nson also e', 'etworks sha', 'llet and je', 'karaoke and', 'el and stoc', 'or hirohito', 'used to ind', 'tball and a', ' trakl kill', 'litical ini', 'fear one ni', 'n awarded t', 'of outer me', 'n most of t', 'nine eight ', ' s took an ', ' charismati', 'iskerdoo ri', 'god s word ', 'roy who is ', 'cs backstag', 'ic overview', ' comes with', 'ition of sp', 'e m s due t', 'air compone', 'all private', 'communist r', 'but observe', 'om acnm acc', ' and submis', 'ocal planes', ' history th', ' centerline', 'o of first ', ' union lead', ' is also po', 'e than any ', 'e ship as a', 'unds such a', 'usiness as ', 'devotional ', 'micronation', 'nd james on', 'f the one n', 'de such dev', 'assical lim', 'mployees of', 'clean as ma']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "\n",
    "    ic = tf.matmul(i, array_ops.concat(concat_dim=1, values=[ix,fx,cx,ox]))\n",
    "    oc = tf.matmul(o, array_ops.concat(concat_dim=1, values=[im,fm,cm,om]))\n",
    "    \n",
    "    iix, ifx, icx, iox = array_ops.split(1, 4, ic)\n",
    "    oim, ofm, ocm, oom = array_ops.split(1, 4, oc)\n",
    "    \n",
    "    input_gate = tf.sigmoid(iix + oim + ib)\n",
    "    forget_gate = tf.sigmoid(ifx + ofm + fb)\n",
    "    output_gate = tf.sigmoid(iox + oom + ob)\n",
    "    \n",
    "    update = icx + ocm + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297579 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.05\n",
      "================================================================================\n",
      "jsutcwro n atzal u j  rx m px d ks qbgnwswqhpdstpysvbips eleupnptapr rqpavfzrvoi\n",
      "cq  mcepmoiccm sg k ghrjlia sxlvmesip eikethbeu ipqa rtsk fdv ntmdxh o fjsnloaof\n",
      "iupx  z ioao xlkcohttoil emdrhehd xnxehngnrgpkaxi yse  ze  zegsiseedwqvdheineoi \n",
      "xpxbaohcvfuckflealp pet  ntrftfdarbpdevme pus asuabrr lc  rre eielme oehiheb  sn\n",
      "mynayjp vrtu awpneocylg tbi gse n nuerkelgbeqlstenhje uqzysooitkla mqd qcl jrtwa\n",
      "================================================================================\n",
      "Validation set perplexity: 20.20\n",
      "Average loss at step 100: 2.592131 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.54\n",
      "Validation set perplexity: 10.12\n",
      "Average loss at step 200: 2.220939 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.35\n",
      "Validation set perplexity: 8.57\n",
      "Average loss at step 300: 2.065133 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 400: 1.971151 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.15\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 500: 1.917636 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 600: 1.873452 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 700: 1.825563 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 800: 1.808805 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 900: 1.776784 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1000: 1.758852 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "================================================================================\n",
      "jo one fer hovernative to much in this were arount for dow dared the rither to t\n",
      "n of the compurimule calben condergand to its in ectail idendies one nine five o\n",
      "ive for two in at inturant in one five one three five three and two source were \n",
      "fter can be conservation of parside empates of nater the yeeneration subcimiem e\n",
      "rage of one nine four sign nice ridise one nine eight six to in of for lenng nar\n",
      "================================================================================\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1100: 1.741469 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1200: 1.732192 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1300: 1.731045 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 1400: 1.709027 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 1500: 1.697061 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 1600: 1.691405 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 1700: 1.669422 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 1800: 1.664150 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 1900: 1.644922 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2000: 1.644785 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "================================================================================\n",
      "rical bo afrian but the pyritishi genth and arrieb to only to even norrackelt ap\n",
      "hritulis and yeasing on the menis sand arantise parts ets reign phy read concors\n",
      "al but soquets on the fonyoms as not the t on the sumpers and instanding homerga\n",
      "k darangly d have abod on one nine seven zero ever by yound battrad a standept c\n",
      "rement han c arst nover commineed ase janneve of a rosside was it indiace goened\n",
      "================================================================================\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2100: 1.651479 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 2200: 1.639204 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2300: 1.619433 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2400: 1.627528 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 2500: 1.655266 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 2600: 1.630468 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2700: 1.616268 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 2800: 1.608586 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 2900: 1.612336 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 3000: 1.607769 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "ramed on intenduted two dayor one nine seative are the subs prolbero ay ad legal\n",
      "ventury thop wextes scoubarn pispologican lost trade of member one nine nine fou\n",
      "y herigguri rom sear and proto or shused many and intelation deton is becomes fo\n",
      "ques by see atstest gumes musiching willion subeb insteadratiogh and social is t\n",
      "y in propert fundralishan creatant theer but now lols pogial intist and other fl\n",
      "================================================================================\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 3100: 1.612994 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 3200: 1.608517 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 3300: 1.606580 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 3400: 1.615199 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 3500: 1.607594 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 3600: 1.623144 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 3700: 1.622406 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 3800: 1.613342 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 3900: 1.598275 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 4000: 1.600879 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "fective and dunents in a classic the yearth been indinent repucinated interapted\n",
      "d as b person europeanists whom juba hubelbieds the edepted in cuality of maydem\n",
      "zard the mon subfrizks vikitives last with the emphess growth cane and its ecoot\n",
      "l hightork four the obcents for demantification rouking the poters to lick s bel\n",
      "rick in theoky upuchiltions recorts have by and orcoults wincehrado edical of th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 4100: 1.597856 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 4200: 1.585611 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 4300: 1.582583 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 4400: 1.572899 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 4500: 1.580723 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 4600: 1.569038 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 4700: 1.577655 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 4800: 1.576550 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 4900: 1.576642 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5000: 1.579731 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "================================================================================\n",
      "s the viroum reparective s rlogham and two zerylea or post brangex lettercrited \n",
      "ja greeth day populaties dun island of fasti wold and two whyet the turries is e\n",
      "and both there c six one nine eight six seven eight gamestlar problem ksi carric\n",
      "bront crisant for one eight three nine zero selam shorte mencative is googs spac\n",
      "t invelvative specially scage sefteds diansuism a call the pound of can contcome\n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5100: 1.571221 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 5200: 1.548849 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 5300: 1.543523 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 5400: 1.557981 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 5500: 1.541594 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 5600: 1.559112 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 5700: 1.545669 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 5800: 1.553410 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 5900: 1.560376 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 6000: 1.545729 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "paros open servery can assamicans aday desetumed mal passist the mide of the con\n",
      "jogement and letting and the cities probrart external land who footse kully on o\n",
      "ky in s caganoleter aboxizhing chresters after not y on my chileate partic rispa\n",
      "tion of in the neneral mine is it some site of daasion crust of fensting was nat\n",
      "anced hebrating and the greempa known divised for excaper occurs the sky were nr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 6100: 1.550389 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 6200: 1.560112 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 6300: 1.559301 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 6400: 1.559570 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 6500: 1.564142 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 6600: 1.576898 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 6700: 1.557045 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 6800: 1.556739 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 6900: 1.558986 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 7000: 1.542797 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "y while the ng of many longer enveros larges meanines kik the black cornet tomel\n",
      "nises it early astradian and and though led those which written it is back or co\n",
      "lus european brithmre examplay recedund france some buse schools two in angenna \n",
      "c its departed and onotests arried the civile of noveler includities inficated h\n",
      "zateds who home insuber while eacy into the follows when a custs turning within \n",
      "================================================================================\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 7100: 1.540710 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 3.98\n",
      "Average loss at step 7200: 1.555728 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 7300: 1.561085 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 7400: 1.567295 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 7500: 1.580395 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 7600: 1.559701 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 7700: 1.566166 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 7800: 1.558530 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 7900: 1.570531 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 8000: 1.590112 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "prismergand the contin roabies of this are from ark mechand banyllifle puacy web\n",
      "ed in one nine seven one fwell stript for dates to solutional european of on fro\n",
      "zers had bhint s year for melten fe is adil tuma biro six countation and interra\n",
      "ver shikfari such forcentions or as also usuals forceft indled tune is restance \n",
      "x numbers moantat of one each single kad diadays constituting under exeroup shop\n",
      "================================================================================\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 8100: 1.575093 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 8200: 1.560444 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 8300: 1.558948 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 8400: 1.555189 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 8500: 1.560039 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 8600: 1.545463 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 8700: 1.553568 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 8800: 1.551458 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 8900: 1.544704 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 9000: 1.541419 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "k b esonom but paralled u ngan bon nates railing roos under stronged the sair ha\n",
      "k factions when recent televal giunese the soungel note pruprieed the entrum isq\n",
      "es stared on f latter are assumed on spockey geaning secret afericar its a norda\n",
      "o positions joy in exterinology one nine chanain run donetearia to barga gan bea\n",
      "varinally act alexbests the agrietes the hames and more companged theor south it\n",
      "================================================================================\n",
      "Validation set perplexity: 4.00\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "    \n",
    "      #print (predictions.shape, predictions)\n",
    "      #print (labels.shape, labels)\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import sys\n",
    "\n",
    "def dump_obj(obj, fname):\n",
    "    try:\n",
    "        f = file(fname, 'wb')\n",
    "        cPickle.dump(obj, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0\n",
    "\n",
    "def load_obj(fname):\n",
    "    try:\n",
    "        f = file(fname, 'rb')\n",
    "        loaded_obj = cPickle.load(f)\n",
    "        f.close()\n",
    "        return loaded_obj\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_lstm_cell(i, o, state):\n",
    "\n",
    "    ic = tf.matmul(i, array_ops.concat(concat_dim=1, values=[ix,fx,cx,ox]))\n",
    "    oc = tf.matmul(o, array_ops.concat(concat_dim=1, values=[im,fm,cm,om]))\n",
    "    \n",
    "    iix, ifx, icx, iox = array_ops.split(1, 4, ic)\n",
    "    oim, ofm, ocm, oom = array_ops.split(1, 4, oc)\n",
    "    \n",
    "    input_gate = tf.sigmoid(iix + oim + ib)\n",
    "    forget_gate = tf.sigmoid(ifx + ofm + fb)\n",
    "    output_gate = tf.sigmoid(iox + oom + ob)\n",
    "    \n",
    "    update = icx + ocm + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import compress\n",
    "from IPython import display\n",
    "from nltk import bigrams\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import math\n",
    "\n",
    "def bigram(text, overlap=False):\n",
    "    \"\"\"Convert a string of text to a list of bigrams\"\"\"\n",
    "    bg = bigrams(text)\n",
    "    if overlap: return [''.join(t) for t in bg]\n",
    "    else: return [''.join(t) for i, t in enumerate(bg) if i%2==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000000\n"
     ]
    }
   ],
   "source": [
    "bg = bigram(text)\n",
    "print(len(bg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' a', 'na', 'rc', 'hi', 'sm', ' o', 'ri', 'gi', 'na', 'te', 'd ', 'as', ' a']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg[:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' anarchism originated as a term of abuse first used against '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(bg[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 729\n",
      "Most common words (+UNK) [['UNK', 0], ('e ', 1843425), (' t', 1224131), ('s ', 1111188), ('th', 990343)]\n",
      "Sample data [5, 96, 220, 75, 267, 10, 56, 196, 96, 29] ===  anarchism originate\n"
     ]
    }
   ],
   "source": [
    "voc_size = 9001\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(voc_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "datum, count, dictionary, reverse_dictionary = build_dataset(bg)\n",
    "voc_size = len(dictionary)\n",
    "\n",
    "vocabulary_size = voc_size\n",
    "\n",
    "print('Dictionary size:', voc_size)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', datum[:10], '===', ''.join([reverse_dictionary[t] for t in datum[:10]]))\n",
    "\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_batch_skipgram(dat, batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span) # used for collecting data[data_index] in the sliding window\n",
    "    for _ in range(span):\n",
    "            buffer.append(dat[data_index])\n",
    "            data_index = (data_index + 1) % len(dat)\n",
    "    for i in range(batch_size // num_skips):\n",
    "            target = skip_window  # target label at the center of the buffer\n",
    "            targets_to_avoid = [ skip_window ]\n",
    "            for j in range(num_skips):\n",
    "                while target in targets_to_avoid:\n",
    "                    target = random.randint(0, span - 1)\n",
    "                targets_to_avoid.append(target)\n",
    "                batch[i * num_skips + j] = buffer[skip_window]\n",
    "                labels[i * num_skips + j, 0] = buffer[target]\n",
    "            buffer.append(dat[data_index])\n",
    "            data_index = (data_index + 1) % len(dat)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "architecture = 'SKIP'\n",
    "\n",
    "batch_size = 512\n",
    "embedding_size = 64 # Dimension of the embedding vector.\n",
    "skip_window = 4 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "if architecture == 'CBOW':\n",
    "    input_shape = [batch_size, num_skips]\n",
    "    generate_batch = generate_batch_cbow\n",
    "elif architecture == 'SKIP':\n",
    "    input_shape = [batch_size]\n",
    "    generate_batch = generate_batch_skipgram\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "  # Input data.\n",
    "  train_dataset = tf.placeholder(tf.int32, shape=input_shape)\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "  softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([voc_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([voc_size]))\n",
    "  \n",
    "  if architecture == 'SKIP':\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    \n",
    "  elif architecture == 'CBOW':\n",
    "    emb = tf.zeros([batch_size, embedding_size])\n",
    "    for j in range(num_skips):\n",
    "        emb += tf.nn.embedding_lookup(embeddings, train_dataset[:, j])\n",
    "    embed = emb\n",
    "\n",
    "  loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,\n",
    "                                                   train_labels, num_sampled, voc_size))  \n",
    "\n",
    "  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "  \n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "    normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 20000: 3.836166\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0QAAAJoCAYAAABGACHXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABgZglEAADAzPr/AeAl5h5uwyd3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3769c47550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to ea: oi, oo, es, dg, oq, af, cp, vf,\n",
      "Nearest to an: ul, by, un, xm, ws, iw, dd, te,\n",
      "Nearest to l : vp, sm, et, dj, uv, bd, nn, fh,\n",
      "Nearest to  c: ec,  p, gn,  s, fv,  g, cs, ex,\n",
      "Nearest to wo: qk, qx, bv, he, zc, fx, fw,  z,\n",
      "Nearest to it: vu, iu, bh, uf, hi, rm, db, z ,\n",
      "Nearest to ns: gy, sn, ll, fd, is, rq, tr, ny,\n",
      "Nearest to si: vt, fl, iu, qd, b , bm, lt, yl,\n",
      "Nearest to fi: ze, mz, ff, yl, nv, af, xd, ui,\n",
      "Nearest to ma: ia, do, kp, pa, rs, sw, yj, np,\n",
      "Nearest to h : sq, w , xs, zv, tn, yx, qn, ps,\n",
      "Nearest to t : fv, pv, vk, ts, xq, qv, s ,  x,\n",
      "Nearest to ha: qz, nx, ww, uk, bn, hi, hr, cb,\n",
      "Nearest to s : jx, a , g , wz, bs, vq, tz, t ,\n",
      "Nearest to ta: la, ra, rp, sl, zv, rc, ry, bt,\n",
      "Nearest to th: a , pv, nl, bw, mb, nz, wh, ms,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "lh = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "        \n",
    "    batch_data, batch_labels = generate_batch(datum, batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    \n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    \n",
    "    average_loss += l\n",
    "    \n",
    "    if step % 2000 == 0:\n",
    "      display.clear_output(wait=True)\n",
    "      if step > 0:\n",
    "        average_loss = average_loss / 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step %d: %f' % (step, average_loss))\n",
    "      lh.append(average_loss)\n",
    "      average_loss = 0\n",
    "            \n",
    "      plt.figure(figsize=(14, 10))\n",
    "\n",
    "      plt.title(\"Training loss, step {}/{}\".format(step, num_steps))\n",
    "      plt.xlabel(\"#step\")\n",
    "      plt.ylabel(\"loss\")\n",
    "      plt.plot(lh, 'b')\n",
    "      plt.show()\n",
    "      \n",
    "\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log = '%s %s,' % (log, close_word)\n",
    "        print(log)\n",
    "        \n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump_obj(BI_EMB, 'embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obj = load_obj('embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BI_EMB = final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALID_SIZE = 1000\n",
    "\n",
    "# model parameters\n",
    "BATCH_SIZE = 256\n",
    "NUM_UNROLLINGS = 10\n",
    "NUM_NODES = 128\n",
    "SUMMARY_FREQUENCY = 100\n",
    "EMBEDDING_DIMENSION = 64\n",
    "DROPOUT_PROBABILITY = 0.5\n",
    "\n",
    "CHARACTER_SIZE = (len(string.ascii_lowercase) + 1)  # [a-z] + ' '\n",
    "VOCABULARY_SIZE = CHARACTER_SIZE ** 2  # [a-z] + ' ' (bigram)\n",
    "\n",
    "FIRST_LETTER = ord(string.ascii_lowercase[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - FIRST_LETTER + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "\n",
    "def bigram2id(bigram):\n",
    "    \"\"\"easily extensible to ngram2id actually\"\"\"\n",
    "    assert len(bigram) == 2, 'Input needs to be 2 characters.'\n",
    "    char_id = 0\n",
    "\n",
    "    for digit, char in enumerate(bigram):\n",
    "        char_id += char2id(char) * (CHARACTER_SIZE ** digit)\n",
    "\n",
    "    return char_id\n",
    "\n",
    "\n",
    "def id2char(char_id):\n",
    "    if char_id > 0:\n",
    "        return chr(char_id + FIRST_LETTER - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "def id2bigram(char_id):\n",
    "    first_digit_id = char_id % CHARACTER_SIZE\n",
    "    second_digit_id = char_id // CHARACTER_SIZE\n",
    "\n",
    "    return id2char(first_digit_id) + id2char(second_digit_id)\n",
    "\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, VOCABULARY_SIZE), dtype=np.float)\n",
    "\n",
    "        for batch_id in range(self._batch_size):\n",
    "            batch[batch_id, bigram2id(self._text[self._cursor[batch_id]: self._cursor[batch_id] + 2])] = 1.0\n",
    "            self._cursor[batch_id] = (self._cursor[batch_id] + 2) % (self._text_size - 1)\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "\n",
    "        self._last_batch = batches[-1]\n",
    "\n",
    "        return batches\n",
    "\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(char_id) for char_id in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def bigrams(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    bigrams back into its (most likely) bigram representation.\"\"\"\n",
    "    return [id2bigram(bigram_id) for bigram_id in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    string = [''] * batches[0].shape[0]\n",
    "\n",
    "    for batch in batches:\n",
    "        string = [''.join(string_tuple) for string_tuple in zip(string, characters(batch))]\n",
    "\n",
    "    return string\n",
    "\n",
    "\n",
    "def log_prob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized probabilities.\"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "\n",
    "        if s >= r:\n",
    "            return i\n",
    "\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, VOCABULARY_SIZE], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, VOCABULARY_SIZE])\n",
    "    return b / np.sum(b, 1)[:, None]\n",
    "\n",
    "\n",
    "def bigram_label2unigram_label(bigram_one_hot_encodings, batch_size=BATCH_SIZE):\n",
    "    unigram_id_labels = np.where(bigram_one_hot_encodings == 1)[1] // CHARACTER_SIZE\n",
    "    return np.array([[float(char_id == unigram_id_labels[batch_id]) for char_id in range(CHARACTER_SIZE)] for batch_id in range(batch_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def TrainBigramLSTM(text, nsteps, embeddings=None):\n",
    "\n",
    "    valid_text = text[:VALID_SIZE]\n",
    "    train_text = text[VALID_SIZE:]\n",
    "    train_size = len(train_text)\n",
    "\n",
    "    train_batches = BatchGenerator(train_text, BATCH_SIZE, NUM_UNROLLINGS)\n",
    "    valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "    # simple LSTM Model\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Parameters for input, forget, cell state, and output gates\n",
    "        W_lstm = tf.Variable(tf.truncated_normal([EMBEDDING_DIMENSION + NUM_NODES, NUM_NODES * 4]))\n",
    "        b_lstm = tf.Variable(tf.zeros([1, NUM_NODES * 4]))\n",
    "\n",
    "        # Variables saving state across unrollings.\n",
    "        previous_output = tf.Variable(tf.zeros([BATCH_SIZE, NUM_NODES]), trainable=False)\n",
    "        previous_state = tf.Variable(tf.zeros([BATCH_SIZE, NUM_NODES]), trainable=False)\n",
    "\n",
    "        # Classifier weights and biases.\n",
    "        W = tf.Variable(tf.truncated_normal([NUM_NODES, VOCABULARY_SIZE], -0.1, 0.1))\n",
    "        b = tf.Variable(tf.zeros([VOCABULARY_SIZE]))\n",
    "\n",
    "        # embedding\n",
    "        if embeddings is not None:\n",
    "            print('Loading embeddings with shape {} ...'.format(embeddings.shape))\n",
    "            embeddings = tf.Variable(embeddings)\n",
    "        else:\n",
    "            embeddings = tf.Variable(tf.random_uniform([VOCABULARY_SIZE, EMBEDDING_DIMENSION], minval=-1.0, maxval=1.0))\n",
    "\n",
    "        # Definition of the cell computation.\n",
    "        def lstm_cell(X, output, state):\n",
    "\n",
    "            X_output = tf.concat(1, [X, output])\n",
    "            all_logits = tf.matmul(X_output, W_lstm) + b_lstm\n",
    "\n",
    "            input_gate = tf.sigmoid(all_logits[:, :NUM_NODES])\n",
    "            forget_gate = tf.sigmoid(all_logits[:, NUM_NODES: NUM_NODES * 2])\n",
    "            output_gate = tf.sigmoid(all_logits[:, NUM_NODES * 2: NUM_NODES * 3])\n",
    "            temp_state = all_logits[:, NUM_NODES * 3:]\n",
    "            state = forget_gate * state + input_gate * tf.tanh(temp_state)\n",
    "\n",
    "            return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "        # Input data.\n",
    "        train_X = list()\n",
    "        train_labels = list()\n",
    "        for _ in range(NUM_UNROLLINGS):\n",
    "            train_X.append(tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1]))\n",
    "            train_labels.append(tf.placeholder(tf.float32, shape=[BATCH_SIZE, VOCABULARY_SIZE]))\n",
    "\n",
    "        # Unrolled LSTM loop.\n",
    "        outputs = list()\n",
    "        output = previous_output\n",
    "        state = previous_state\n",
    "\n",
    "        for X in train_X:\n",
    "            embed = tf.reshape(tf.nn.embedding_lookup(embeddings, X), shape=[BATCH_SIZE, -1])\n",
    "            output, state = lstm_cell(embed, output, state)\n",
    "            outputs.append(output)\n",
    "\n",
    "        # State saving across unrollings.\n",
    "        with tf.control_dependencies([previous_output.assign(output), previous_state.assign(state)]):\n",
    "            # Classifier.\n",
    "            logits = tf.nn.xw_plus_b(tf.nn.dropout(tf.concat(0, outputs), DROPOUT_PROBABILITY), W, b)\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "\n",
    "        # Optimizer.\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "        # Predictions.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        # Sampling and validation eval: batch 1, no unrolling.\n",
    "        sample_input = tf.placeholder(tf.int32, shape=[1, 1])\n",
    "        sample_embed = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input), shape=[1, -1])\n",
    "        previous_sample_output = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "        previous_sample_state = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "        reset_sample_state = tf.group(previous_sample_output.assign(tf.zeros([1, NUM_NODES])), previous_sample_state.assign(tf.zeros([1, NUM_NODES])))\n",
    "        sample_output, sample_state = lstm_cell(sample_embed, previous_sample_output, previous_sample_state)\n",
    "\n",
    "        with tf.control_dependencies([previous_sample_output.assign(sample_output), previous_sample_state.assign(sample_state)]):\n",
    "            sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, W, b))\n",
    "\n",
    "\n",
    "    # Run the model\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        lh = []\n",
    "        tf.initialize_all_variables().run()\n",
    "        print('Initialized')\n",
    "        mean_loss = 0\n",
    "\n",
    "        for step in range(nsteps):\n",
    "            batches = train_batches.next()\n",
    "            feed_dict = dict()\n",
    "\n",
    "            for batch_id in range(NUM_UNROLLINGS):\n",
    "                feed_dict[train_X[batch_id]] = np.where(batches[batch_id] == 1)[1].reshape((-1, 1))\n",
    "                feed_dict[train_labels[batch_id]] = batches[batch_id + 1]\n",
    "\n",
    "            _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "            mean_loss += l\n",
    "\n",
    "            if step % SUMMARY_FREQUENCY == 0:\n",
    "                if step > 0:\n",
    "                    mean_loss = mean_loss / SUMMARY_FREQUENCY\n",
    "                display.clear_output(wait=True)\n",
    "                lh.append(mean_loss)\n",
    "                # The mean loss is an estimate of the loss over the last few batches.\n",
    "                print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "                \n",
    "                plt.figure(figsize=(14, 10))\n",
    "                plt.title(\"Training loss, step {}/{}\".format(step, nsteps))\n",
    "                plt.xlabel(\"step\")\n",
    "                plt.ylabel(\"loss\")\n",
    "                plt.plot(lh, 'b')\n",
    "                plt.show()\n",
    "\n",
    "                mean_loss = 0\n",
    "                labels = np.concatenate([batch for batch in batches[1:]])\n",
    "                #print('Minibatch perplexity: %.2f' % float(np.exp(log_prob(predictions, labels))))\n",
    "\n",
    "                if step % (SUMMARY_FREQUENCY * 10) == 0:\n",
    "                    # Generate some samples.\n",
    "\n",
    "                    for _ in range(3):\n",
    "                        feed = sample(random_distribution())\n",
    "                        sentence = bigrams(feed)[0]\n",
    "                        reset_sample_state.run()\n",
    "\n",
    "                        for _ in range(69):\n",
    "                            feed = np.where(feed == 1)[1].reshape((-1, 1))\n",
    "                            prediction = sample_prediction.eval({sample_input: feed})\n",
    "                            feed = sample(prediction)\n",
    "                            sentence += ''.join(bigrams(feed))\n",
    "                            last_bigram_id = bigram2id(sentence[-2:])\n",
    "                            feed = np.array([[float(last_bigram_id == bigram_id) for bigram_id in range(VOCABULARY_SIZE)]])                    \n",
    "                    \n",
    "                print('=' * 80)\n",
    "                print('Networks says:', sentence)\n",
    "                print('=' * 80)\n",
    "\n",
    "                # Measure validation set perplexity.\n",
    "                reset_sample_state.run()\n",
    "                valid_log_prob = 0\n",
    "\n",
    "                for _ in range(VALID_SIZE):\n",
    "                    valid_batch = valid_batches.next()\n",
    "                    predictions = sample_prediction.eval({sample_input: np.where(valid_batch[0] == 1)[1].reshape((-1, 1))})\n",
    "                    valid_log_prob = valid_log_prob + log_prob(predictions, valid_batch[1])\n",
    "\n",
    "                print('Validation set perplexity: %.2f' % float(np.exp(valid_log_prob / VALID_SIZE)))\n",
    "                \n",
    "    return session, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 19900: 3.373253 learning rate: 0.010000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAAJoCAYAAABcPMFZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAgISABAAAAADJ/wM5o/wYmRZfiwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3768e52050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Networks says: succomplarenlf celts in is paec ortacre orlimex s an tute of thedor origing ladeugh jx aftrogrited coluttion wroina the manyly partard of eu\n",
      "================================================================================\n",
      "Validation set perplexity: 19.64\n"
     ]
    }
   ],
   "source": [
    "sesh = TrainBigramLSTM(text, 20000, BI_EMB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
