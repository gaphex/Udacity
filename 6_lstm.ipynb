{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "8tQJd2YSCfWR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "from tensorflow.python.ops import array_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [],
   "source": [
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "num_unrollings=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'bout it in ', 'nomination ', 'lower here ', 'when milita', 'ogenetic ap', ' three nine', 'unced that ', 'lleria arch', 'any he did ', 'reviated as', 'unce the wo', ' abbeys and', 'one a a gen', 'shing the r', 's of it to ', 'married urr', 'ouncil twic', 'sity upset ', 'toon advent', 'hel and ric', 't used the ', 'ased in the', 'settled the', 'y and litur', 'nes flight ', ' disgust be', 'tury the wa', 'ay opened f', ' were a tri', 'society and', 'hree mm zer', 'tion from t', 'y the human', 'ago based c', 'each the dh', 'migration t', 's one zero ', ' zero zero ', 'rick and al', 'new york ot', 'ycles have ', 'short subje', 'sample code', 'he boeing s', 'em a year l', 'sgow two yo', 'ne nine eig', 'e listed wi', 'ill even mo', 'lt during t', 'ine one it ', 'eber has pr', ' x three an', ' not dead n', 'f the team ', 'o be made t', 'hy essentia', 'll s enthus', 'unds the mo', 'yer who rec', ' a step or ', 'operates th', 's has been ', 'ore signifi', ' community ', 'rmines secu', 'se were pub', 'a fierce cr', 'often used ', ' fuel extra', 'h are used ', ' two six ei', 'tland which', 'ature that ', 'e three two', 'aristotle s', ' about one ', 'e dragas co', 'hree five f', 'ity can be ', ' popularity', 'ecombinant ', 're both wor', ' and intrac', 'believed to', 'tensive man', 'established', 'tion of the', 'sored by th', 'he attack f', 'l elements ', 'dy to pass ', 'the number ', 'ed to bring', 'king the en', 'f certain d', 'yed in ezek', 'french jans', 'vance gba d', 'at it will ', 'racter dies', 'tion from e', 'inating it ', 'e convince ', 'm the john ', 'ither spont', 'their praye', 'ent told hi', 'ee six whic', 'argest part', 'jebas balen', 'ampaign and', 'wo zero th ', 'ce in a spe', 'aryotic org', 'rver side s', ' ripening o', 'gain the am', 'he agency t', 'ious texts ', 'ands entire', ' assignment', 'hifley labo', 'o capitaliz', 'is pronounc', 'rettas fran', ' other exot', 'a duplicate', ' disks with', 'former is w', ' two six on', 'gh ann es d', 'in a song f', 'g the serie', 'such as rid', 'ine january', 'g should be', 'ar it is po', 'ections of ', 'ross zero t', ' for exampl', 's worldwide', 'y knowledge', 'cal theorie', 'ro ad by co', 'f an inch t', 'nds ed robe', 'ast instanc', 'ends to des', 'e phantom a', 'gives out t', ' dimensiona', 'ithdrew bra', 'as pi appro', ' was a tire', 'most holy m', 'uman story ', 'fice did no', ' telecommun', 't s support', 'iod of tran', 'olunteers o', 'melville s ', 'u is still ', 'tures in on', 'e end of wo', ' campaignin', 'e oscillati', ' zero zero ', 'tain its su', 'ed on colon', 'o eight sub', 'chusetts me', 'normal cour', 'od of war e', 'of italy la', 'ith which h', 'e of the le', ' software p', 's the tower', 'and written', 'e icj repor', 'respond exa', 'klahoma pre', 'ar kinds of', ' most of th', 'ed him to c', 'erprise lin', 'e six large', ' running of', 'ica parts o', 'ws becomes ', 'ine six nin', 'lization ab', 'in february', 'et in a naz', 'e seven the', 'ey accepted', 'ife of grod', 'the fabian ', 'an t be see', ' help when ', 'ons this pr', 'etchy to re', 'n systems a', 'ng bombs wi', 'press robin', ' sharman ne', ' robbe gril', 'laces for k', ' long barre', 'ised empero', 'liably be u', 'erican foot', 'factor but ', 'ting in pol', 'four zero f', 'nd has been', 'he theory o', 'd neo latin', 'erger one n', 'seco the a ', 'rong was a ', 'th risky ri', 'they hear g', 'er donna tr', 'm paramedic', 'encyclopedi', 'ndows also ', 'rall defini', 'city is one', 'fense the a', 'struction a', 'ecause of c', 'three one b', 'duating fro', ' grappling ', 'ifferent fo', ' doomed it ', 'treet grid ', 'r four zero', 'n militant ', 'or by food ', 'ations more', 'studies the', 'etal compou', ' in show bu', 'appeal of d', ' types of m', 'rs larry an', 'me three of', 'si have mad', ' called cla', 'ncluding em', 'ficult to c']\n",
      "['ists advoca', ' part two c', ' gore s end', ' the whole ', 'ary governm', 'pproach two', 'e one six z', ' atheism wo', 'hes nationa', ' not protes', 's dr mr and', 'ord america', 'd monasteri', 'neral inver', 'right of ap', ' make the d', 'raca prince', 'ce in their', ' the devils', 'tures had p', 'chard baer ', ' rom routin', 'e st family', 'e arctic fo', 'rgical lang', ' six two fi', 'ecause of t', 'aters were ', 'for passeng', 'ibe of sout', 'd that this', 'ro five in ', 'the nationa', 'ns ash init', 'chess recor', 'harma he wa', 'took place ', ' zero zero ', ' five yaniv', 'lfred rosen', 'ther well k', ' small whee', 'ect college', 'e for c and', 'seven six s', 'later moore', 'oung white ', 'ght zero s ', 'ith a gloss', 'ore content', 'this period', ' was known ', 'robably bee', 'nd x one or', 'naturally a', ' s success ', 'to recogniz', 'ally elemen', 'siastic bac', 'ost promine', 'ceived the ', ' two toward', 'hree submar', ' enjoyed an', 'icant than ', ' college ne', 'urity of th', 'blished as ', 'ritic of th', ' in a more ', 'acted from ', ' to this da', 'ight in sig', 'h then beca', ' was attack', 'o four ocke', 's uncaused ', ' zero zero ', 'onstantine ', 'four two vo', ' lost as in', 'y and moral', ' region and', 'rking on de', 'cellular ic', 'o be import', 'nufacturing', 'd lyell s c', 'e size of t', 'he russell ', 'from hyrsyl', ' or trachei', ' him a stic', ' q by one p', 'g good fort', 'nd of his p', 'drugs confu', 'kiel two ei', 'senist theo', 'dragon ball', ' take to co', 's all items', 'euclidean g', ' and the me', ' the priest', ' connally u', 'taneously o', 'ers a force', 'im to name ', 'ch became t', 'tner of the', 'ngues and b', 'd barred at', ' century ea', 'ecial cell ', 'ganelles ma', 'standard fo', 'of fruit th', 'mplified si', 'to change t', ' such as es', 'ely upon th', 't of number', 'or governme', 'ze on the g', 'ced and tha', 'ncis poulen', 'tic technol', 'e of the or', 'h questiona', 'widely used', 'ne nine nin', 'd hiver one', 'for sinatra', 'es to a sev', 'ddles child', 'y eight mar', 'e moved so ', 'ossible the', ' this artic', 'the lead ch', 'le a carrie', 'e after eng', 'e of the em', 'es classica', 'onstantine ', 'they were f', 'ert silverb', 'ce the non ', 'scribe deat', 'appear on s', 'the award i', 'al analysis', 'andenburg f', 'ox frac the', 'eless worke', 'mormons bel', ' from histo', 'ot become f', 'nications i', 't or at lea', 'nsformation', 'originally ', ' lifetime e', ' disagreed ', 'ne nine two', 'orld war ii', 'ng in recog', 'ing system ', ' five isbn ', 'urface area', 'nial troops', 'btypes base', 'erely looke', 'rse of stud', 'eventually ', 'anguages th', 'he or she a', 'eadership a', 'patent worl', 'r commissio', 'n after the', 'rts seven s', 'actly to th', 'ess one nin', 'f insurance', 'he indictee', 'carefully o', 'nux suse li', 'est cities ', 'f the house', 'of kingston', ' the first ', 'ne get back', 'bout the pe', 'y one six e', 'zi concentr', 'e country y', 'd a proposa', 'dno and lit', ' society ne', 'en the wond', ' the queen ', 'revented th', 'elatively s', 'according t', 'ith global ', 'nson also e', 'etworks sha', 'llet and je', 'karaoke and', 'el and stoc', 'or hirohito', 'used to ind', 'tball and a', ' trakl kill', 'litical ini', 'fear one ni', 'n awarded t', 'of outer me', 'n most of t', 'nine eight ', ' s took an ', ' charismati', 'iskerdoo ri', 'god s word ', 'roy who is ', 'cs backstag', 'ic overview', ' comes with', 'ition of sp', 'e m s due t', 'air compone', 'all private', 'communist r', 'but observe', 'om acnm acc', ' and submis', 'ocal planes', ' history th', ' centerline', 'o of first ', ' union lead', ' is also po', 'e than any ', 'e ship as a', 'unds such a', 'usiness as ', 'devotional ', 'micronation', 'nd james on', 'f the one n', 'de such dev', 'assical lim', 'mployees of', 'clean as ma']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "\n",
    "    ic = tf.matmul(i, array_ops.concat(concat_dim=1, values=[ix,fx,cx,ox]))\n",
    "    oc = tf.matmul(o, array_ops.concat(concat_dim=1, values=[im,fm,cm,om]))\n",
    "    \n",
    "    iix, ifx, icx, iox = array_ops.split(1, 4, ic)\n",
    "    oim, ofm, ocm, oom = array_ops.split(1, 4, oc)\n",
    "    \n",
    "    input_gate = tf.sigmoid(iix + oim + ib)\n",
    "    forget_gate = tf.sigmoid(ifx + ofm + fb)\n",
    "    output_gate = tf.sigmoid(iox + oom + ob)\n",
    "    \n",
    "    update = icx + ocm + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296965 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.03\n",
      "================================================================================\n",
      "r e noovaw  npbnacesomrtyrduot tliefxdsn k ksowinoepvvuzr  nshbyynzeo htoenimx h\n",
      "hmzgqoecpmta nirkeiod ogzv tir lycpd wbceh vm d s e qeejt vypodueeapatbvt ouvbzo\n",
      "nd nvts omynenissflv gnolbn ogoid abf eelpcy pc tibx  e leelu xbxwazp xmhdor tkn\n",
      "uiecbyqsdt  pz eecqgeewbeq aphfntaitsaue nrvemedagrxoknozeqaj zwspyldzstoei ngpa\n",
      "k rrn  rygcjjxjsi ceogo df htcpows z nafntlmbdxobqgb xorr e   qavfe xtocbloc jir\n",
      "================================================================================\n",
      "Validation set perplexity: 20.21\n",
      "Average loss at step 100: 2.580279 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.97\n",
      "Validation set perplexity: 10.29\n",
      "Average loss at step 200: 2.223583 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.40\n",
      "Validation set perplexity: 8.67\n",
      "Average loss at step 300: 2.068864 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 400: 1.965543 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 500: 1.908920 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 600: 1.864309 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 700: 1.823810 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 800: 1.804576 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 900: 1.776288 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1000: 1.758602 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "================================================================================\n",
      "horedy ruetmentor of secres dilarational dlitten the one nine two and term deysi\n",
      "by used indeased in greas seas wehled at the canken juluarine the zeined the ass\n",
      "mented infore desim s chereent twen ts intorly a gire vowly gars for iladiev of \n",
      "bagkey beoum ags maing centrab internatiomy abolg kncle to notean follexason lar\n",
      "hages from noty wal mine assurce in the d indeactry the signived compinialole th\n",
      "================================================================================\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1100: 1.743232 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1200: 1.734465 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 1300: 1.732642 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1400: 1.712459 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 1500: 1.701088 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 1600: 1.696323 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 1700: 1.672768 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 1800: 1.669992 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 1900: 1.647912 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2000: 1.648625 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "================================================================================\n",
      "histrias to the covines enective agognital nothing days usitamed in the recame d\n",
      "d in overn of to but transaic meoused ip had trons areblevity the controcal resu\n",
      "janpper jempre ba people with skrocient bloum amonn nooge gency smpp pircusselis\n",
      "nated time three zero houthn equical in however high siension of qubstration to \n",
      "may was seets areinallenis oncy it mermandar attorulsy the euppination aboup and\n",
      "================================================================================\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2100: 1.656806 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2200: 1.643637 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2300: 1.623883 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2400: 1.633190 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 2500: 1.657229 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 2600: 1.633859 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 2700: 1.618908 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 2800: 1.612166 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 2900: 1.611250 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 3000: 1.610318 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "ing is roanal resushe hondwing his borni while stristar vise that u newsy coladl\n",
      "x visertly and perreary equeare and wasy the germans larkenk has wends and first\n",
      "rilio doed in wincioths pstazed plotaced thesed therelisagest is communitisional\n",
      "th in one nine six seven four franccidementury siics the one one three two des c\n",
      "j mater tholpholly vilge cringed and relativisms sabeak egi with capt in one sm \n",
      "================================================================================\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 3100: 1.615676 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 3200: 1.608667 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 3300: 1.609104 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 3400: 1.617045 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 3500: 1.610060 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 3600: 1.624251 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 3700: 1.623797 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 3800: 1.614764 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 3900: 1.600705 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 4000: 1.601370 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "batoram ria spoples a covs enstaces one two four three zero threy district one t\n",
      "y the issulfer of polumpers of hollishean pronols flouded movicate be els her wh\n",
      " of entayality with active sire manories age extangerraty to asotth one nine emi\n",
      "ja bwo dk vealgam fazing pates karddour winder which the flark country rivel seq\n",
      "vives for dause arbing rights when the catuannes examphor new stauolo fill tass \n",
      "================================================================================\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 4100: 1.596269 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 4200: 1.585405 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 4300: 1.581132 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 4400: 1.573881 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 4500: 1.582059 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 4600: 1.569855 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 4700: 1.580529 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 4800: 1.579323 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 4900: 1.574597 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5000: 1.578771 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "================================================================================\n",
      "ord takely for it isbrapolation history to the goo varkers obligitus in fisting \n",
      "fire of the old clib a lanon while the ising orgintly of jaik be mon aircript ru\n",
      "man oundent was senotions one nine eight seven zero nine nine one wile agninatio\n",
      "joret electan writtens a organisecleances guan the pohicly latin its a b one nin\n",
      "th and was amador at the be central stredalts acceptin all planable one nine zer\n",
      "================================================================================\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5100: 1.571647 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5200: 1.546853 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 5300: 1.542551 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 5400: 1.555406 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 5500: 1.540571 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 5600: 1.561346 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 5700: 1.546839 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 5800: 1.554263 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 5900: 1.561649 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 6000: 1.546855 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "================================================================================\n",
      "winkfo a his bill age beaising considerate inlist siserned entrienced a pank nit\n",
      "ing the can ginom a games his catenation sobus origibs is nd guiderch the first \n",
      "ber that whilph the after first the one six to r n dutch betweats the upine from\n",
      "pited raffon gru reyary semitated for shirt and films bead daul attempted by tac\n",
      "lation are clearer but dunicians little a feweren to also receigand engrantian c\n",
      "================================================================================\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 6100: 1.550598 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 6200: 1.561550 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 6300: 1.557833 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 6400: 1.559155 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 6500: 1.565316 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 6600: 1.575669 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 6700: 1.556435 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 6800: 1.557831 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 6900: 1.559906 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 7000: 1.540874 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "================================================================================\n",
      "ngedithers male uponds dectory contrillions if on economiel as evisoral until wi\n",
      "fire five he vary this who scienced by is long pagh notilled part ad tradition u\n",
      "x with the ulos archabpelol while in a monegy b oppassion on zero docn politics \n",
      "ensing lictable not of time scholuct of aeshetade profusts fatter as an the one \n",
      "ess and the ontertonedn authoritary londent andlstination s despected at haskpt \n",
      "================================================================================\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 7100: 1.540186 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 7200: 1.555738 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 7300: 1.559985 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 7400: 1.564296 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 7500: 1.580606 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 7600: 1.557799 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 7700: 1.565704 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 7800: 1.557402 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 7900: 1.569781 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 8000: 1.588226 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "quasing of electronal sjemes casowing a peoficuly the alen waber jevica to manan\n",
      "pister uid but assame founder and zew iss plays of a people leagy scripted to mo\n",
      "jogravia onlin one one zero strenque the anarchisms to examplets startam formmym\n",
      "g fide on the rexicated are operat player for s indapecy two zero zero repeely m\n",
      "x has nation of a segnee where the firms one eight estiette an intereral subsiag\n",
      "================================================================================\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 8100: 1.574292 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 8200: 1.558923 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 8300: 1.557062 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 8400: 1.556300 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 8500: 1.559558 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 8600: 1.546207 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 8700: 1.551839 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 8800: 1.551391 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 8900: 1.545705 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 9000: 1.541109 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "x excupts wealty a monizage ipdn or in main contrcs bodo k forest cuttor in segi\n",
      "urrief in there in sender of the prespitated nove and two four maiss me syller i\n",
      "s control susried low britishs as azonds brow with is respech economics characte\n",
      "led christian meniozes purpurer in only ebrices in erecued for relobable libfrel\n",
      " to offer veryallym and one six seven seven five he was rote of through a regard\n",
      "================================================================================\n",
      "Validation set perplexity: 4.07\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "    \n",
    "      #print (predictions.shape, predictions)\n",
    "      #print (labels.shape, labels)\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import sys\n",
    "\n",
    "def dump_obj(obj, fname):\n",
    "    try:\n",
    "        f = file(fname, 'wb')\n",
    "        cPickle.dump(obj, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0\n",
    "\n",
    "def load_obj(fname):\n",
    "    try:\n",
    "        f = file(fname, 'rb')\n",
    "        loaded_obj = cPickle.load(f)\n",
    "        f.close()\n",
    "        return loaded_obj\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_lstm_cell(i, o, state):\n",
    "\n",
    "    ic = tf.matmul(i, array_ops.concat(concat_dim=1, values=[ix,fx,cx,ox]))\n",
    "    oc = tf.matmul(o, array_ops.concat(concat_dim=1, values=[im,fm,cm,om]))\n",
    "    \n",
    "    iix, ifx, icx, iox = array_ops.split(1, 4, ic)\n",
    "    oim, ofm, ocm, oom = array_ops.split(1, 4, oc)\n",
    "    \n",
    "    input_gate = tf.sigmoid(iix + oim + ib)\n",
    "    forget_gate = tf.sigmoid(ifx + ofm + fb)\n",
    "    output_gate = tf.sigmoid(iox + oom + ob)\n",
    "    \n",
    "    update = icx + ocm + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import compress\n",
    "from IPython import display\n",
    "from nltk import bigrams as tobigrams\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import math\n",
    "\n",
    "def to_bigrams(text, overlap=False):\n",
    "    \"\"\"Convert a string of text to a list of bigrams\"\"\"\n",
    "    bg = tobigrams(text)\n",
    "    if overlap: return [''.join(t) for t in bg]\n",
    "    else: return [''.join(t) for i, t in enumerate(bg) if i%2==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999999\n"
     ]
    }
   ],
   "source": [
    "bg = to_bigrams(text, overlap=True)\n",
    "print(len(bg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' a', 'an', 'na', 'ar', 'rc', 'ch', 'hi', 'is', 'sm', 'm ', ' o', 'or', 'ri']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg[:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 729\n",
      "Most common words (+UNK) [['UNK', 0], ('e ', 3686256), (' t', 2449254), ('s ', 2222333), ('th', 1980538)]\n",
      "Sample data [5, 14, 97, 31, 218, 79, 75, 33, 267, 92] ===  aannaarrcchhiissmm \n"
     ]
    }
   ],
   "source": [
    "voc_size = 9001\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(voc_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "datum, count, dictionary, reverse_dictionary = build_dataset(bg)\n",
    "voc_size = len(dictionary)\n",
    "\n",
    "vocabulary_size = voc_size\n",
    "\n",
    "print('Dictionary size:', voc_size)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', datum[:10], '===', ''.join([reverse_dictionary[t] for t in datum[:10]]))\n",
    "\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_batch_skipgram(dat, batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span) # used for collecting data[data_index] in the sliding window\n",
    "    for _ in range(span):\n",
    "            buffer.append(dat[data_index])\n",
    "            data_index = (data_index + 1) % len(dat)\n",
    "    for i in range(batch_size // num_skips):\n",
    "            target = skip_window  # target label at the center of the buffer\n",
    "            targets_to_avoid = [ skip_window ]\n",
    "            for j in range(num_skips):\n",
    "                while target in targets_to_avoid:\n",
    "                    target = random.randint(0, span - 1)\n",
    "                targets_to_avoid.append(target)\n",
    "                batch[i * num_skips + j] = buffer[skip_window]\n",
    "                labels[i * num_skips + j, 0] = buffer[target]\n",
    "            buffer.append(dat[data_index])\n",
    "            data_index = (data_index + 1) % len(dat)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "architecture = 'SKIP'\n",
    "\n",
    "batch_size = 512\n",
    "embedding_size = 64 # Dimension of the embedding vector.\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "if architecture == 'CBOW':\n",
    "    input_shape = [batch_size, num_skips]\n",
    "    generate_batch = generate_batch_cbow\n",
    "elif architecture == 'SKIP':\n",
    "    input_shape = [batch_size]\n",
    "    generate_batch = generate_batch_skipgram\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "  # Input data.\n",
    "  train_dataset = tf.placeholder(tf.int32, shape=input_shape)\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "  softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([voc_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([voc_size]))\n",
    "  \n",
    "  if architecture == 'SKIP':\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    \n",
    "  elif architecture == 'CBOW':\n",
    "    emb = tf.zeros([batch_size, embedding_size])\n",
    "    for j in range(num_skips):\n",
    "        emb += tf.nn.embedding_lookup(embeddings, train_dataset[:, j])\n",
    "    embed = emb\n",
    "\n",
    "  loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,\n",
    "                                                   train_labels, num_sampled, voc_size))  \n",
    "\n",
    "  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "  \n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "    normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 50000: 1.527570\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0QAAAJoCAYAAABGACHXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f483cbd4c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to  a:  t, ea,  i,  b,  o, fa,  w,  u,\n",
      "Nearest to fo: fa, fu, fi, fr, bo, ho, xg, fs,\n",
      "Nearest to  t:  a,  i, gk,  c,  b,  w,  q, dt,\n",
      "Nearest to at: dt, ht, rt, ct, xt, nt, pt, xd,\n",
      "Nearest to  e: gj, xe, ye, oe,  p, kw,  b, we,\n",
      "Nearest to na: nm, ne, nw, ni, nk, qv, sa, oa,\n",
      "Nearest to in: on, yn, wn, ii, id, un, ic, it,\n",
      "Nearest to to: io, wo, tr, ho, th, ti, tu, lo,\n",
      "Nearest to or: ur, ar, oc, ou, oi, ir, on, tq,\n",
      "Nearest to as: ns, es, rs, ys, ad, is, gs, av,\n",
      "Nearest to ha: hi, ho, hu, ja, hy, ka, he, fa,\n",
      "Nearest to m : kq, ms, h , mp, md, fd, g , z ,\n",
      "Nearest to  d:  q, bq, dd, cd,  z, nd,  j,  c,\n",
      "Nearest to ea: oa, eg, ei, ev, la,  a, ec, ej,\n",
      "Nearest to  o:  h, zf,  c,  i,  m,  a, mo,  w,\n",
      "Nearest to ed: sd, md, nd, e , ew, kc, ex, em,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 50001\n",
    "lh = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "        \n",
    "    batch_data, batch_labels = generate_batch(datum, batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    \n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    \n",
    "    average_loss += l\n",
    "    \n",
    "    if step % 2000 == 0:\n",
    "      display.clear_output(wait=True)\n",
    "      if step > 0:\n",
    "        average_loss = average_loss / 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step %d: %f' % (step, average_loss))\n",
    "      lh.append(average_loss)\n",
    "      average_loss = 0\n",
    "            \n",
    "      plt.figure(figsize=(14, 10))\n",
    "\n",
    "      plt.title(\"Training loss, step {}/{}\".format(step, num_steps))\n",
    "      plt.xlabel(\"#step\")\n",
    "      plt.ylabel(\"loss\")\n",
    "      plt.plot(lh, 'b')\n",
    "      plt.show()\n",
    "      \n",
    "\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log = '%s %s,' % (log, close_word)\n",
    "        print(log)\n",
    "        \n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump_obj(final_embeddings, 'embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BI_EMB = load_obj('embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALID_SIZE = 1000\n",
    "\n",
    "# model parameters\n",
    "BATCH_SIZE = 256\n",
    "NUM_UNROLLINGS = 10\n",
    "NUM_NODES = 256\n",
    "SUMMARY_FREQUENCY = 100\n",
    "EMBEDDING_DIMENSION = 64\n",
    "DROPOUT_PROBABILITY = 0.34\n",
    "\n",
    "CHARACTER_SIZE = (len(string.ascii_lowercase) + 1)  # [a-z] + ' '\n",
    "VOCABULARY_SIZE = CHARACTER_SIZE ** 2  # [a-z] + ' ' (bigram)\n",
    "\n",
    "FIRST_LETTER = ord(string.ascii_lowercase[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - FIRST_LETTER + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "\n",
    "def bigram2id(bigram):\n",
    "    \"\"\"easily extensible to ngram2id actually\"\"\"\n",
    "    assert len(bigram) == 2, 'Input needs to be 2 characters.'\n",
    "    char_id = 0\n",
    "\n",
    "    for digit, char in enumerate(bigram):\n",
    "        char_id += char2id(char) * (CHARACTER_SIZE ** digit)\n",
    "\n",
    "    return char_id\n",
    "\n",
    "\n",
    "def id2char(char_id):\n",
    "    if char_id > 0:\n",
    "        return chr(char_id + FIRST_LETTER - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "def id2bigram(char_id):\n",
    "    first_digit_id = char_id % CHARACTER_SIZE\n",
    "    second_digit_id = char_id // CHARACTER_SIZE\n",
    "\n",
    "    return id2char(first_digit_id) + id2char(second_digit_id)\n",
    "\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, VOCABULARY_SIZE), dtype=np.float)\n",
    "\n",
    "        for batch_id in range(self._batch_size):\n",
    "            batch[batch_id, bigram2id(self._text[self._cursor[batch_id]: self._cursor[batch_id] + 2])] = 1.0\n",
    "            self._cursor[batch_id] = (self._cursor[batch_id] + 2) % (self._text_size - 1)\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "\n",
    "        self._last_batch = batches[-1]\n",
    "\n",
    "        return batches\n",
    "\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(char_id) for char_id in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def bigrams(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    bigrams back into its (most likely) bigram representation.\"\"\"\n",
    "    return [id2bigram(bigram_id) for bigram_id in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    string = [''] * batches[0].shape[0]\n",
    "\n",
    "    for batch in batches:\n",
    "        string = [''.join(string_tuple) for string_tuple in zip(string, characters(batch))]\n",
    "\n",
    "    return string\n",
    "\n",
    "\n",
    "def log_prob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized probabilities.\"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "\n",
    "        if s >= r:\n",
    "            return i\n",
    "\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, VOCABULARY_SIZE], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, VOCABULARY_SIZE])\n",
    "    return b / np.sum(b, 1)[:, None]\n",
    "\n",
    "\n",
    "def bigram_label2unigram_label(bigram_one_hot_encodings, batch_size=BATCH_SIZE):\n",
    "    unigram_id_labels = np.where(bigram_one_hot_encodings == 1)[1] // CHARACTER_SIZE\n",
    "    return np.array([[float(char_id == unigram_id_labels[batch_id]) for char_id in range(CHARACTER_SIZE)] for batch_id in range(batch_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def TrainBigramLSTM(text, nsteps, embeddings=None):\n",
    "    \n",
    "    valid_text = text[:VALID_SIZE]\n",
    "    train_text = text[VALID_SIZE:]\n",
    "    train_size = len(train_text)\n",
    "\n",
    "    train_batches = BatchGenerator(train_text, BATCH_SIZE, NUM_UNROLLINGS)\n",
    "    valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "    # simple LSTM Model\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Parameters for input, forget, cell state, and output gates\n",
    "        W_lstm = tf.Variable(tf.truncated_normal([EMBEDDING_DIMENSION + NUM_NODES, NUM_NODES * 4]))\n",
    "        b_lstm = tf.Variable(tf.zeros([1, NUM_NODES * 4]))\n",
    "\n",
    "        # Variables saving state across unrollings.\n",
    "        previous_output = tf.Variable(tf.zeros([BATCH_SIZE, NUM_NODES]), trainable=False)\n",
    "        previous_state = tf.Variable(tf.zeros([BATCH_SIZE, NUM_NODES]), trainable=False)\n",
    "\n",
    "        # Classifier weights and biases.\n",
    "        W = tf.Variable(tf.truncated_normal([NUM_NODES, VOCABULARY_SIZE], -0.1, 0.1))\n",
    "        b = tf.Variable(tf.zeros([VOCABULARY_SIZE]))\n",
    "\n",
    "        # embedding\n",
    "        if embeddings is not None:\n",
    "            print('Loading embeddings with shape {} ...'.format(embeddings.shape))\n",
    "            embeddings = tf.Variable(embeddings)\n",
    "        else:\n",
    "            embeddings = tf.Variable(tf.random_uniform([VOCABULARY_SIZE, EMBEDDING_DIMENSION], minval=-1.0, maxval=1.0))\n",
    "\n",
    "        # Definition of the cell computation.\n",
    "        def lstm_cell(X, output, state):\n",
    "\n",
    "            X_output = tf.concat(1, [X, output])\n",
    "            all_logits = tf.matmul(X_output, W_lstm) + b_lstm\n",
    "\n",
    "            input_gate = tf.sigmoid(all_logits[:, :NUM_NODES])\n",
    "            forget_gate = tf.sigmoid(all_logits[:, NUM_NODES: NUM_NODES * 2])\n",
    "            output_gate = tf.sigmoid(all_logits[:, NUM_NODES * 2: NUM_NODES * 3])\n",
    "            temp_state = all_logits[:, NUM_NODES * 3:]\n",
    "            state = forget_gate * state + input_gate * tf.tanh(temp_state)\n",
    "\n",
    "            return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "        # Input data.\n",
    "        train_X = list()\n",
    "        train_labels = list()\n",
    "        for _ in range(NUM_UNROLLINGS):\n",
    "            train_X.append(tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1]))\n",
    "            train_labels.append(tf.placeholder(tf.float32, shape=[BATCH_SIZE, VOCABULARY_SIZE]))\n",
    "\n",
    "        # Unrolled LSTM loop.\n",
    "        outputs = list()\n",
    "        output = previous_output\n",
    "        state = previous_state\n",
    "\n",
    "        for X in train_X:\n",
    "            embed = tf.reshape(tf.nn.embedding_lookup(embeddings, X), shape=[BATCH_SIZE, -1])\n",
    "            output, state = lstm_cell(embed, output, state)\n",
    "            outputs.append(output)\n",
    "\n",
    "        # State saving across unrollings.\n",
    "        with tf.control_dependencies([previous_output.assign(output), previous_state.assign(state)]):\n",
    "            # Classifier.\n",
    "            logits = tf.nn.xw_plus_b(tf.nn.dropout(tf.concat(0, outputs), DROPOUT_PROBABILITY), W, b)\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "\n",
    "        # Optimizer.\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "        # Predictions.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        # Sampling and validation eval: batch 1, no unrolling.\n",
    "        sample_input = tf.placeholder(tf.int32, shape=[1, 1])\n",
    "        sample_embed = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input), shape=[1, -1])\n",
    "        previous_sample_output = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "        previous_sample_state = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "        reset_sample_state = tf.group(previous_sample_output.assign(tf.zeros([1, NUM_NODES])), previous_sample_state.assign(tf.zeros([1, NUM_NODES])))\n",
    "        sample_output, sample_state = lstm_cell(sample_embed, previous_sample_output, previous_sample_state)\n",
    "\n",
    "        with tf.control_dependencies([previous_sample_output.assign(sample_output), previous_sample_state.assign(sample_state)]):\n",
    "            sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, W, b))\n",
    "\n",
    "\n",
    "    # Run the model\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        lh = []\n",
    "        tf.initialize_all_variables().run()\n",
    "        print('Initialized')\n",
    "        mean_loss = 0\n",
    "\n",
    "        for step in range(nsteps):\n",
    "            batches = train_batches.next()\n",
    "            feed_dict = dict()\n",
    "\n",
    "            for batch_id in range(NUM_UNROLLINGS):\n",
    "                feed_dict[train_X[batch_id]] = np.where(batches[batch_id] == 1)[1].reshape((-1, 1))\n",
    "                feed_dict[train_labels[batch_id]] = batches[batch_id + 1]\n",
    "\n",
    "            _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "            mean_loss += l\n",
    "\n",
    "            if step % SUMMARY_FREQUENCY == 0:\n",
    "                if step > 0:\n",
    "                    mean_loss = mean_loss / SUMMARY_FREQUENCY\n",
    "                display.clear_output(wait=True)\n",
    "                lh.append(mean_loss)\n",
    "                # The mean loss is an estimate of the loss over the last few batches.\n",
    "                print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "                \n",
    "                plt.figure(figsize=(14, 10))\n",
    "                plt.title(\"Training loss, step {}/{}\".format(step, nsteps))\n",
    "                plt.xlabel(\"step\")\n",
    "                plt.ylabel(\"loss\")\n",
    "                plt.plot(lh, 'b')\n",
    "                plt.show()\n",
    "\n",
    "                mean_loss = 0\n",
    "                labels = np.concatenate([batch for batch in batches[1:]])\n",
    "                #print('Minibatch perplexity: %.2f' % float(np.exp(log_prob(predictions, labels))))\n",
    "\n",
    "                \n",
    "                if step % (SUMMARY_FREQUENCY * 10) == 0:\n",
    "                    # Generate some samples.\n",
    "                    thoughts = []\n",
    "\n",
    "                    for _ in range(3):\n",
    "                        feed = sample(random_distribution())\n",
    "                        sentence = bigrams(feed)[0]\n",
    "                        reset_sample_state.run()\n",
    "\n",
    "                        for _ in range(140):\n",
    "                            feed = np.where(feed == 1)[1].reshape((-1, 1))\n",
    "                            prediction = sample_prediction.eval({sample_input: feed})\n",
    "                            feed = sample(prediction)\n",
    "                            sentence += ''.join(bigrams(feed))\n",
    "                            last_bigram_id = bigram2id(sentence[-2:])\n",
    "                            feed = np.array([[float(last_bigram_id == bigram_id) for bigram_id in range(VOCABULARY_SIZE)]])                    \n",
    "                        thoughts.append(sentence)\n",
    "                    \n",
    "                print('=' * 80)\n",
    "                print('Networks says:', '\\n'.join(thoughts))\n",
    "                print('=' * 80)\n",
    "\n",
    "                # Measure validation set perplexity.\n",
    "                reset_sample_state.run()\n",
    "                valid_log_prob = 0\n",
    "\n",
    "                for _ in range(VALID_SIZE):\n",
    "                    valid_batch = valid_batches.next()\n",
    "                    predictions = sample_prediction.eval({sample_input: np.where(valid_batch[0] == 1)[1].reshape((-1, 1))})\n",
    "                    valid_log_prob = valid_log_prob + log_prob(predictions, valid_batch[1])\n",
    "\n",
    "                print('Validation set perplexity: %.2f' % float(np.exp(valid_log_prob / VALID_SIZE)))\n",
    "                \n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 50000: 3.540694 learning rate: 0.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAAJoCAYAAABcPMFZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f483c924d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Networks says: wer legeut of for senia crisbanecal courival compeecaustion the persred for an later it had anke ud daeu the osh prole chop rive bjofence veaiyt prefeelliome inriances nal thnist to dito when bodyoril eous an and of majos noth injindime moderabarnne exus huma and wa super genhal in\n",
      "ge from with the populion with writters into and like withirted ton links five ever for more at chroot epitaded the oepend somating to loanichage producst iq le seapsed based by the hat red the skun production his work used thed progratuswotion reprita were in the matures addariall\n",
      "gkiricer of the plosottals hea of only jrations to the rightent feate ar from one km that agued inditudarligs ast of all five don to the whots hom linal fores the oras hymechurned from the st ndy but is one of the manded by up being vif per the valycwer attionato af wife north and \n",
      "================================================================================\n",
      "Validation set perplexity: 22.10\n"
     ]
    }
   ],
   "source": [
    "sesh = TrainBigramLSTM(text, 50001, BI_EMB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Networks says: xs loss of the movain theory gain the termle lessed to purow a pularle spuy not year as yeard dot wrarget zero rearts other of eight kine red bostine a declude was and secths also was the loty is to three met of per he unist freque velifare his momenpo a many zighmod by city known \n",
    "l a showle successor a use roder ged for five davers politicbally to the for debouting to listactions and bartic site hilo glor and use major fasbar and scheleriverfan servial four with for official indrien reaniver stat vickl an more chanifimarious lign nabicilon first to inad whe\n",
    "lifornralone place frang eleouse mpaad that the numbing make and a capous deved shovibin ipoinguctives fated betweal from presidation rh electuring in serven graft world ne neq k many god king links fil condition atire empember be sageam the m it from v of prittors in her nided it \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
